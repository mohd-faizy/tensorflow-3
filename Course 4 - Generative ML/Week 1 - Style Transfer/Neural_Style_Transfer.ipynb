{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neural Style Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyftRTSMuwue"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "import IPython.display as display_obj\n",
        "from random import randint\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U9It5Ii2Oof"
      },
      "source": [
        "##Download Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeXebYusyHwC"
      },
      "source": [
        "Download images and choose a style image and a content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqc0OJHwyFAk"
      },
      "source": [
        "#https://cdn.pixabay.com/photo/2017/02/28/23/00/swan-2107052_1280.jpg\n",
        "\n",
        "!wget  https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\n",
        "!wget  https://cdn.pixabay.com/photo/2017/02/28/23/00/swan-2107052_1280.jpg\n",
        "!wget  https://i.dawn.com/large/2019/10/5db6a03a4c7e3.jpg\n",
        "!wget  https://cdn.pixabay.com/photo/2015/09/22/12/21/rudolph-951494_1280.jpg\n",
        "!wget https://cdn.pixabay.com/photo/2015/10/13/02/59/animals-985500_1280.jpg\n",
        "\n",
        "_, content_path = os.path.split(\"https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\")\n",
        "_, style_path = os.path.split(\"https://cdn.pixabay.com/photo/2015/09/22/12/21/rudolph-951494_1280.jpg\")\n",
        "#style_path = tf.keras.utils.get_file('style_image.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Visualize the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klh6ObK2t_vH"
      },
      "source": [
        "Define a function to load an image and limit its maximum dimension to 512 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6VEGrGLh62"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor_shape = tf.shape(tensor)\n",
        "  number_elem_shape = tf.shape(tensor_shape)\n",
        "  if number_elem_shape > 3:\n",
        "    assert tensor_shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(tensor) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZqDcTWD51d0"
      },
      "source": [
        "def preprocess_image(image):\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "    return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAlRzJZrWM3"
      },
      "source": [
        "Create a simple function to display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBX-eNT8PAK_"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfaEcRubyuWO"
      },
      "source": [
        "def show_images_with_objects(images, titles=[]):\n",
        "\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6yADTcux1FZ"
      },
      "source": [
        "def load_images(content_path, style_path):\n",
        "  content_image = load_img(\"{}\".format(content_path))\n",
        "  style_image = load_img(\"{}\".format(style_path))\n",
        "\n",
        "  return content_image, style_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "## Build the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-tASys0eJv"
      },
      "source": [
        "Choose intermediate layers from the network to represent the style and content of the image:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfX_6iA0WAX"
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer of interest\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx"
      },
      "source": [
        "def vgg_model(layer_names):\n",
        "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "  # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  print(vgg.input)\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "vgg = vgg_model(style_layers + content_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaIvZf5wWn_"
      },
      "source": [
        "And to create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv8hZU0oKIm_"
      },
      "source": [
        "def get_style_loss(features, targets):\n",
        "  # \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
        "  # # height, width, num filters of each layer\n",
        "  # # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
        "  # height, width, channels = base_style.get_shape().as_list()\n",
        "  # gram_style = gram_matrix(base_style)\n",
        "  return tf.reduce_mean(tf.square(features - targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et8M1lOgKL8o"
      },
      "source": [
        "def get_content_loss(features, targets):\n",
        "  return tf.reduce_mean(tf.square(features - targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAy1iGPdoEpZ"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTK5qzG_MKh"
      },
      "source": [
        "def get_style_image_features(image):  \n",
        "  preprocessed_style_image = preprocess_image(image)\n",
        "  style_outputs = vgg(preprocessed_style_image)#\n",
        "  gram_style_features = []\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs[:num_style_layers]]\n",
        "  #style_features = [tf.reshape(style_layer, shape=tf.shape(style_layer)[1:]) for style_layer in style_outputs[:num_style_layers]]\n",
        "  #style_features = gram_matrix(style_features)\n",
        "  return gram_style_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rq02U9_a6L"
      },
      "source": [
        "def get_content_image_features(image):\n",
        "\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "  vgg_outputs = vgg(preprocessed_content_image)#\n",
        "\n",
        "  content_features = [content_layer for content_layer in vgg_outputs[num_style_layers:]]\n",
        "\n",
        "  return content_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q20XhIHnotQA"
      },
      "source": [
        "def get_style_content_loss(style_targets, style_outputs, content_targets, content_outputs, style_weight, content_weight):\n",
        "  style_loss = tf.add_n([ get_style_loss(style_output, style_target)\n",
        "                           for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "  style_loss *= style_weight / num_style_layers\n",
        "\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target)\n",
        "                           for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "  content_loss *= content_weight / num_content_layers\n",
        "  loss = style_loss + content_loss\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgpTJwL_vE2"
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp2g2tI58RI0"
      },
      "source": [
        "def calculate_gradients(image, content_targets, style_targets, style_weight, content_weight,with_regularization=False ):\n",
        "    total_variation_weight = 30\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      style_features = get_style_image_features(image * 255)\n",
        "      content_features = get_content_image_features(image * 255)\n",
        "      loss = get_style_content_loss(style_targets, style_features, content_targets, content_features, style_weight, content_weight)\n",
        "      if with_regularization:\n",
        "        loss += total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "    gradients = tape.gradient(loss, image)\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-MPRxuGp-5A"
      },
      "source": [
        "def update_image_with_style(image, content_targets, style_targets, optimizer, style_weight, content_weight, with_regularization=False):\n",
        "  \n",
        "  gradients = calculate_gradients(image, content_targets, style_targets, style_weight, content_weight, with_regularization)\n",
        "  optimizer.apply_gradients([(gradients, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTOpNNw2Wp2"
      },
      "source": [
        "##Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Btr_j9M1gu"
      },
      "source": [
        "def fit_style_transfer(input_image, style_image, optimizer, epochs=1, steps_per_epoch=1, with_regularization=False, style_weight = 0.01):\n",
        "\n",
        "  images = []\n",
        "  import time\n",
        "  start = time.time()\n",
        "\n",
        "  step = 0\n",
        "\n",
        "  #style_weight=1.0\n",
        "  content_weight=1e2\n",
        "\n",
        "  style_targets = get_style_image_features(style_image)\n",
        "  content_targets = get_content_image_features(input_image)\n",
        "\n",
        "\n",
        "  input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
        "  \n",
        "  input_image = tf.Variable(input_image) \n",
        "  images.append(tf.Variable(input_image)) \n",
        "  \n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "      update_image_with_style(input_image, content_targets, style_targets, optimizer, style_weight, content_weight, with_regularization=with_regularization)\n",
        "\n",
        "      print(\".\", end='')\n",
        "      if (m + 1) % 10 == 0:\n",
        "        images.append(tf.Variable(input_image))\n",
        "    \n",
        "    display_obj.clear_output(wait=True)\n",
        "    display_image = tensor_to_image(input_image)\n",
        "\n",
        "    \n",
        "    display_obj.display(display_image)\n",
        "    images.append(tf.Variable(input_image))\n",
        "    print(\"Train step: {}\".format(step))\n",
        "  end = time.time()\n",
        "  print(\"Total time: {:.1f}\".format(end-start)) \n",
        "  \n",
        "  return input_image, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcOmo92OyABa"
      },
      "source": [
        "\n",
        "content_image, style_image = load_images(\"swan-2107052_1280.jpg\", \"animals-985500_1280.jpg\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQW1tXYoLbUS"
      },
      "source": [
        "weight =  0.001 #@param {type:\"number\"}\n",
        "adam = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "stylized_image, display_images = fit_style_transfer(input_image=content_image, style_image=style_image, optimizer=adam, epochs=10, steps_per_epoch=100, style_weight=weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Qiu1hyy4W3"
      },
      "source": [
        "#@title (RUN ME!) Display Utilities\n",
        "\n",
        "import imageio\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image\n",
        "\n",
        "def display_gif(GIF_PATH):\n",
        "  with open(GIF_PATH,'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "def create_gif(images):\n",
        "  GIF_PATH = \"/content/{}.gif\".format(randint(0, 10000))\n",
        "  imageio.mimsave(GIF_PATH, images, fps=1)\n",
        "  return GIF_PATH\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFMUQ_wJnWp",
        "cellView": "both"
      },
      "source": [
        "#@title (RUN ME!) Display GIF of Intermedite Outputs\n",
        "gif_images = [np.squeeze(image.numpy(), axis=0) for image in display_images]\n",
        "gif_path = create_gif(gif_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mixNaLAl6FUO"
      },
      "source": [
        "display_gif(gif_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVB3anJMY2v"
      },
      "source": [
        "## Total variation loss\n",
        "\n",
        "One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TrAkGDH_U97"
      },
      "source": [
        "#@title (RUN ME!)Plot Utilities\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var\n",
        "\n",
        "def plot_deltas_for_single_image(x_deltas, y_deltas, name=\"Original\", row=1):\n",
        "  plt.figure(figsize=(14,10))\n",
        "  plt.subplot(row,2,1)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: {}\".format(name))\n",
        "\n",
        "  plt.subplot(row,2,2)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: {}\".format(name))\n",
        "\n",
        "def plot_deltas(original_image_deltas, stylized_image_deltas):\n",
        "  orig_x_deltas, orig_y_deltas = original_image_deltas\n",
        "  \n",
        "  stylized_x_deltas, stylized_y_deltas = stylized_image_deltas\n",
        "\n",
        "  plot_deltas_for_single_image(orig_x_deltas, orig_y_deltas, name=\"Original\")\n",
        "  plot_deltas_for_single_image(stylized_x_deltas, stylized_y_deltas, name=\"Stylized Image\", row=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn67NdjAR2xr"
      },
      "source": [
        "#@title (RUN ME!)Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(stylized_image)\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTessd-DCdcC"
      },
      "source": [
        "## Re-run the optimization\n",
        "\n",
        "Choose a weight for the `total_variation_loss`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dPRr8BqexB"
      },
      "source": [
        "variation_model_weight =   0.001#@param {type:\"number\"}\n",
        "\n",
        "stylized_image1, display_images1 = fit_style_transfer(input_image=content_image, style_image=style_image, optimizer=adam, epochs=10, steps_per_epoch=100, with_regularization=True, style_weight=variation_model_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pul5V0ig5PKS",
        "cellView": "both"
      },
      "source": [
        "#@title (RUN ME!)Display GIF\n",
        "\n",
        "gif_images1 = [np.squeeze(image.numpy(), axis=0) for image in display_images1]\n",
        "gif_path1 = create_gif(gif_images1)\n",
        "display_gif(gif_path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lla8IAunRviU",
        "cellView": "form"
      },
      "source": [
        "#@title (RUN ME!)Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(stylized_image)\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6h-0EaCD_P"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image1], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtMRtWBAz21"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image1], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YZ3XvA5cwx"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "#content_image, style_image = load_images(\"swan-2107052_1280.jpg\", style_path)\n",
        "\n",
        "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\n",
        "stylized_image = hub_module(tf.image.convert_image_dtype(content_image, tf.float32), tf.image.convert_image_dtype(style_image, tf.float32))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
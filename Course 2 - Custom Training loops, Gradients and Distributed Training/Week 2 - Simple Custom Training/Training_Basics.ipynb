{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-3/blob/master/Course%202%20-%20Custom%20Training%20loops%2C%20Gradients%20and%20Distributed%20Training/Week%202%20-%20Simple%20Custom%20Training/Training_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "# Custom Training Basics\n",
    "\n",
    "The purpose of this example is to gain a basic understanding of building custom training loops. It takes you through the underlying logic of fitting any model to a set of inputs and outputs. We will be training our model on the linear equation for a straight line, ax + b. We will implement basic linear regression from scratch using grsdient tape and try to minimize the loss incurred by the model using linear regression.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LXMVuV0VhDr"
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiolgWMPgpwI"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7O6eEGF5DcN"
   },
   "source": [
    "## Define Model\n",
    "\n",
    "We define our model as a class. Here x is our input tensor. The model should output values of **ax+b**. Here we strat off by initializing a and b to random values. During the training process, values of a and b get updated in accordance with linear regression so as to minimize the loss incurred by the model. Once we arrive at optimal values for a and b, the model would have been trained to correctly predict the values of ax+b.\n",
    "\n",
    "Hence, **a** and **b** are trainable weights of the model. **x** is the input and output y = ax + b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WRu7Pze7wk8"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "  def __init__(self):\n",
    "    # Initialize the weights to `2.0` and the bias to `1.0`\n",
    "    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n",
    "    self.w = tf.Variable(2.0)\n",
    "    self.b = tf.Variable(1.0)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.w * x + self.b\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xa6j_yXa-j79"
   },
   "source": [
    "### Define a loss function\n",
    "\n",
    "A loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let's use the standard L2 loss, also known as the least square errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0ysUFGY924U"
   },
   "outputs": [],
   "source": [
    "def loss(predicted_y, target_y):\n",
    "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "### Obtain training data\n",
    "\n",
    "First, synthesize the training data as a line with y=TRUE_w * x + TRUE_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "TRUE_w = 3.0\n",
    "TRUE_b = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "xs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "ys = (TRUE_w * xs) + TRUE_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-50nq-wPBsAW"
   },
   "source": [
    "Before training the model, visualize the loss value by plotting the model's predictions in red and the training data in blue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eb83LtrB4nt"
   },
   "outputs": [],
   "source": [
    "def plot_data(inputs, outputs, predicted_outputs):\n",
    "  real = plt.scatter(inputs, outputs, c='b')\n",
    "  predicted = plt.scatter(inputs, predicted_outputs, c='r')\n",
    "  plt.legend((real,predicted), ('Real Data', 'Predicted Data'))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XL25a_aEOuim"
   },
   "outputs": [],
   "source": [
    "plot_data(xs, ys, model(xs))\n",
    "print('Current loss: %1.6f' % loss(model(xs), ys).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSDP-yeq_4jE"
   },
   "source": [
    "### Define a training loop\n",
    "\n",
    "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the trainable weights **a** and **b** to reduce the loss. There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`â€”our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of `tf.GradientTape` for automatic differentiation and `tf.assign_sub` for decrementing a value (which combines `tf.assign` and `tf.sub`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBIACgdnA55X"
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
    "  model.w.assign_sub(learning_rate * dw)\n",
    "  model.b.assign_sub(learning_rate * db)\n",
    "\n",
    "  return current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Finally, let's repeatedly run through the training data and see how `a` and `b` evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XdfkR223D9dW"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Collect the history of W-values and b-values to plot later\n",
    "list_w, list_b = [], []\n",
    "epochs = range(15)\n",
    "losses = []\n",
    "for epoch in epochs:\n",
    "  list_w.append(model.w.numpy())\n",
    "  list_b.append(model.b.numpy())\n",
    "  current_loss = train(model, xs, ys, learning_rate=0.1)\n",
    "  losses.append(current_loss)\n",
    "  print('Epoch %2d: w=%1.2f b=%1.2f, loss=%2.5f' %\n",
    "        (epoch, list_w[-1], list_b[-1], current_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EI_1PwOBR6TW"
   },
   "source": [
    "In addition to the values for losses, we also plot the progression of trainable variables over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8gJThOCNXAp"
   },
   "outputs": [],
   "source": [
    "  plt.plot(epochs, list_w, 'r',\n",
    "           epochs, list_b, 'b')\n",
    "  plt.plot([TRUE_w] * len(epochs), 'r--',\n",
    "          [TRUE_b] * len(epochs), 'b--')\n",
    "  plt.legend(['w', 'b', 'True w', 'True b'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsTbG9J2MM9W"
   },
   "source": [
    "## Plots for Evaluation\n",
    "Let us plot the actual outputs in red and outputs predicted by model in blue on a set of random test examples.\n",
    "\n",
    "You can see that the model is able to make predictions on the test set fairly accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRHpHCJ3273d"
   },
   "outputs": [],
   "source": [
    "test_inputs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "test_outputs = test_inputs * TRUE_w + TRUE_b\n",
    "\n",
    "predicted_test_outputs = model(test_inputs)\n",
    "plot_data(test_inputs, test_outputs, predicted_test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY-j2FJYSfis"
   },
   "source": [
    "We also visualize the cost function against the values of each of the trainable weights the model approximated to over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY-gQWFfOIu-"
   },
   "outputs": [],
   "source": [
    "def plot_loss_for_weights(weights_list, losses):\n",
    "  for idx, weights in enumerate(weights_list):\n",
    "    plt.subplot(120 + idx + 1)\n",
    "    plt.plot(weights['values'], losses, 'r')\n",
    "    plt.plot(weights['values'], losses, 'bo')\n",
    "    plt.xlabel(weights['name'])\n",
    "    plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeB5HYUILXnN"
   },
   "outputs": [],
   "source": [
    "weights_list = [{ 'name' : \"w\",\n",
    "                  'values' : list_w\n",
    "                },\n",
    "                {\n",
    "                  'name' : \"b\",\n",
    "                  'values' : list_b\n",
    "                }]\n",
    "\n",
    "plot_loss_for_weights(weights_list, losses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Training Basics.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

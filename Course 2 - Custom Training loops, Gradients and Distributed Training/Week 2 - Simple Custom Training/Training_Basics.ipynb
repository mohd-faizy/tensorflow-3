{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Basics.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-3/blob/master/Course%202%20-%20Custom%20Training%20loops%2C%20Gradients%20and%20Distributed%20Training/Week%202%20-%20Simple%20Custom%20Training/Training_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hrXv0rU9sIma"
      },
      "source": [
        "# Custom training: basic\n",
        "The purpose of this example is to gain a basic understanding of building custom training loops. It takes you through the underlying logic of fitting any model to a set of inputs and outputs. We will be training our model on the linear equation for a straight line, ax + b. We will implement basic linear regression from scratch using grsdient tape and try to minimize the loss incurred by the model using linear regression.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3LXMVuV0VhDr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NiolgWMPgpwI",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7O6eEGF5DcN",
        "colab_type": "text"
      },
      "source": [
        "##Define Model\n",
        "We define our model as a class. Here x is our input tensor. The model should output values of **ax+b**. Here we strat off by initializing a and b to random values. During the training process, values of a and b get updated in accordance with linear regression so as to minimize the loss incurred by the model. Once we arrive at optimal values for a and b, the model would have been trained to correctly predict the values of ax+b.\n",
        "\n",
        "Hence, **a** and **b** are trainable weights of the model. **x** is the input and output y = ax + b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WRu7Pze7wk8",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    # Initialize the weights to `2.0` and the bias to `1.0`\n",
        "    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n",
        "    self.w = tf.Variable(2.0)\n",
        "    self.b = tf.Variable(1.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b\n",
        "\n",
        "model = Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xa6j_yXa-j79"
      },
      "source": [
        "### Define a loss function\n",
        "\n",
        "A loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let's use the standard L2 loss, also known as the least square errors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y0ysUFGY924U",
        "colab": {}
      },
      "source": [
        "def loss(predicted_y, target_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qutT_fkl_CBc"
      },
      "source": [
        "### Obtain training data\n",
        "\n",
        "First, synthesize the training data as a line with y=TRUE_w * x + TRUE_b\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gxPTb-kt_N5m",
        "colab": {}
      },
      "source": [
        "TRUE_w = 3.0\n",
        "TRUE_b = 2.0\n",
        "NUM_EXAMPLES = 1000\n",
        "\n",
        "xs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "\n",
        "ys = (TRUE_w * xs) + TRUE_b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-50nq-wPBsAW"
      },
      "source": [
        "Before training the model, visualize the loss value by plotting the model's predictions in red and the training data in blue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_eb83LtrB4nt",
        "colab": {}
      },
      "source": [
        "def plot_data(inputs, outputs, predicted_outputs):\n",
        "  real = plt.scatter(inputs, outputs, c='b')\n",
        "  predicted = plt.scatter(inputs, predicted_outputs, c='r')\n",
        "  plt.legend((real,predicted), ('Real Data', 'Predicted Data'))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL25a_aEOuim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_data(xs, ys, model(xs))\n",
        "print('Current loss: %1.6f' % loss(model(xs), ys).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sSDP-yeq_4jE"
      },
      "source": [
        "### Define a training loop\n",
        "\n",
        "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the trainable weights **a** and **b** to reduce the loss. There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`â€”our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of `tf.GradientTape` for automatic differentiation and `tf.assign_sub` for decrementing a value (which combines `tf.assign` and `tf.sub`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBIACgdnA55X",
        "colab": {}
      },
      "source": [
        "def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "  dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "  model.w.assign_sub(learning_rate * dw)\n",
        "  model.b.assign_sub(learning_rate * db)\n",
        "\n",
        "  return current_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RwWPaJryD2aN"
      },
      "source": [
        "Finally, let's repeatedly run through the training data and see how `a` and `b` evolve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XdfkR223D9dW",
        "colab": {}
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "list_w, list_b = [], []\n",
        "epochs = range(15)\n",
        "losses = []\n",
        "for epoch in epochs:\n",
        "  list_w.append(model.w.numpy())\n",
        "  list_b.append(model.b.numpy())\n",
        "  current_loss = train(model, xs, ys, learning_rate=0.1)\n",
        "  losses.append(current_loss)\n",
        "  print('Epoch %2d: w=%1.2f b=%1.2f, loss=%2.5f' %\n",
        "        (epoch, list_w[-1], list_b[-1], current_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_1PwOBR6TW",
        "colab_type": "text"
      },
      "source": [
        "In addition to the values for losses, we also plot the progression of trainable variables over epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8gJThOCNXAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  plt.plot(epochs, list_w, 'r',\n",
        "           epochs, list_b, 'b')\n",
        "  plt.plot([TRUE_w] * len(epochs), 'r--',\n",
        "          [TRUE_b] * len(epochs), 'b--')\n",
        "  plt.legend(['w', 'b', 'True w', 'True b'])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsTbG9J2MM9W",
        "colab_type": "text"
      },
      "source": [
        "## Plots for Evaluation\n",
        "Let us plot the actual outputs in red and outputs predicted by model in blue on a set of random test examples.\n",
        "\n",
        "You can see that the model is able to make predictions on the test set fairly accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRHpHCJ3273d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "test_outputs = test_inputs * TRUE_w + TRUE_b\n",
        "\n",
        "predicted_test_outputs = model(test_inputs)\n",
        "plot_data(test_inputs, test_outputs, predicted_test_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-j2FJYSfis",
        "colab_type": "text"
      },
      "source": [
        "We also visualize the cost function against the values of each of the trainable weights the model approximated to over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY-gQWFfOIu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss_for_weights(weights_list, losses):\n",
        "  for idx, weights in enumerate(weights_list):\n",
        "    plt.subplot(120 + idx + 1)\n",
        "    plt.plot(weights['values'], losses, 'r')\n",
        "    plt.plot(weights['values'], losses, 'bo')\n",
        "    plt.xlabel(weights['name'])\n",
        "    plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeB5HYUILXnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights_list = [{ 'name' : \"w\",\n",
        "                  'values' : list_w\n",
        "                },\n",
        "                {\n",
        "                  'name' : \"b\",\n",
        "                  'values' : list_b\n",
        "                }]\n",
        "\n",
        "plot_loss_for_weights(weights_list, losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}

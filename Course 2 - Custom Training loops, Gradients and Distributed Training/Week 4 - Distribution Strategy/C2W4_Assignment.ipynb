{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYysdyb-CaWM"
   },
   "source": [
    "# Custom training with tf.distribute.Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MM6W__qraV55"
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmPfSVg_lGlb"
   },
   "outputs": [],
   "source": [
    "# Note, if you get a checksum error when downloading the data\n",
    "# you might need to install tfds-nightly, and then restart \n",
    "# !pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NsM-Bma5wNw"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MqDQO0KCaWS"
   },
   "outputs": [],
   "source": [
    "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
    "\n",
    "(train_examples, validation_examples, test_examples), info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True, split = splits, data_dir='data/')\n",
    "\n",
    "num_examples = info.splits['train'].num_examples\n",
    "num_classes = info.features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AXoHhrsbdF3"
   },
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mVuLZhbem8d"
   },
   "source": [
    "How does `tf.distribute.MirroredStrategy` strategy work?\n",
    "\n",
    "*   All the variables and the model graph is replicated on the replicas.\n",
    "*   Input is evenly distributed across the replicas.\n",
    "*   Each replica calculates the loss and gradients for the input it received.\n",
    "*   The gradients are synced across all the replicas by summing them.\n",
    "*   After the sync, the same update is made to the copies of the variables on each replica.\n",
    "\n",
    "Note: You can put all the code below inside a single scope. We are dividing it into several code cells for illustration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2VeZUWUj5S4"
   },
   "outputs": [],
   "source": [
    "# If the list of devices is not specified in the\n",
    "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZngeM_2o0_JO"
   },
   "outputs": [],
   "source": [
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k53F5I_IiGyI"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qb6nDgxiN_n"
   },
   "source": [
    "Export the graph and the variables to the platform-agnostic SavedModel format. After your model is saved, you can load it with or without the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwJtsCQhHK-E"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = num_examples\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWUl3kUk8D5d"
   },
   "outputs": [],
   "source": [
    "pixels = 224\n",
    "MODULE_HANDLE = 'data/resnet_50_feature_vector'\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHGFit478BWD"
   },
   "outputs": [],
   "source": [
    "def format_image(image, label):\n",
    "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
    "    return  image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7fj3GskHC8g"
   },
   "source": [
    "Create the datasets and distribute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYrMNNDhAvVl"
   },
   "outputs": [],
   "source": [
    "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
    "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
    "test_batches = test_examples.map(format_image).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def distribute_datasets(strategy, train_batches, validation_batches, test_batches):\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    train_dist_dataset = None\n",
    "    val_dist_dataset = None\n",
    "    test_dist_dataset = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return train_dist_dataset, val_dist_dataset, test_dist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_dataset, val_dist_dataset, test_dist_dataset = distribute_datasets(strategy, train_batches, validation_batches, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAXAo_wWbWSb"
   },
   "source": [
    "## Create the model\n",
    "\n",
    "We use the Model Subclassing API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ODch-OFCaW4"
   },
   "outputs": [],
   "source": [
    "class ResNetModel(tf.keras.Model):\n",
    "    def __init__(self, classes):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self._feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
    "                                                 trainable=False) \n",
    "        self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self._feature_extractor(inputs)\n",
    "        x = self._classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iagoTBfijUz"
   },
   "outputs": [],
   "source": [
    "# Create a checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-wlFFZbP33n"
   },
   "source": [
    "## Define the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R144Wci782ix",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
    "    # global batch size.\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE)\n",
    "    # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8y54-o9T2Ni"
   },
   "source": [
    "## Define the metrics to track loss and accuracy\n",
    "\n",
    "These metrics track the test loss and training and test accuracy. You can use `.result()` to get the accumulated statistics at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zt3AHb46Tr3w"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='train_accuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuKuNXPORfqJ"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrMmakq5EqeQ"
   },
   "outputs": [],
   "source": [
    "# model and optimizer must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = ResNetModel(classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUQ_nAP1MtA9"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
    "    with strategy.scope():\n",
    "        def train_step(inputs):\n",
    "            images, labels = inputs\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                ### START CODE HERE ###\n",
    "                predictions = model(None, None)\n",
    "                loss = compute_loss(None, None)\n",
    "                ### END CODE HERE ###\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            train_accuracy.update_state(labels, predictions)\n",
    "            return loss \n",
    "\n",
    "        def test_step(inputs):\n",
    "            images, labels = inputs\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            predictions = model(None, None)\n",
    "            t_loss = loss_object(None, None)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            test_loss.update_state(t_loss)\n",
    "            test_accuracy.update_state(labels, predictions)\n",
    "        \n",
    "        return train_step, test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step, test_step = train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION\n",
    "def distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
    "    with strategy.scope():\n",
    "        @tf.function\n",
    "        def distributed_train_step(dataset_inputs):\n",
    "            ### START CODE HERE ###\n",
    "            per_replica_losses = strategy.run(None, None)\n",
    "            ### END CODE HERE ###\n",
    "            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                                   axis=None)\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_test_step(dataset_inputs):\n",
    "            ### START CODE HERE ###\n",
    "            return strategy.run(None, None)\n",
    "            ### END CODE HERE ###\n",
    "    \n",
    "        return distributed_train_step, distributed_test_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_train_step, distributed_test_step = distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX975dMSNw0e"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAIN LOOP\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x in tqdm(train_dist_dataset):\n",
    "            total_loss += distributed_train_step(x)\n",
    "            num_batches += 1\n",
    "        train_loss = total_loss / num_batches\n",
    "\n",
    "        # TEST LOOP\n",
    "        for x in test_dist_dataset:\n",
    "            distributed_test_step(x)\n",
    "\n",
    "        template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "                    \"Test Accuracy: {}\")\n",
    "        print (template.format(epoch+1, train_loss,\n",
    "                               train_accuracy.result()*100, test_loss.result(),\n",
    "                               test_accuracy.result()*100))\n",
    "\n",
    "        test_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1YvXqOpwy08"
   },
   "source": [
    "Things to note in the example above:\n",
    "\n",
    "* We are iterating over the `train_dist_dataset` and `test_dist_dataset` using  a `for x in ...` construct.\n",
    "* The scaled loss is the return value of the `distributed_train_step`. This value is aggregated across replicas using the `tf.distribute.Strategy.reduce` call and then across batches by summing the return value of the `tf.distribute.Strategy.reduce` calls.\n",
    "* `tf.keras.Metrics` should be updated inside `train_step` and `test_step` that gets executed by `tf.distribute.Strategy.experimental_run_v2`.\n",
    "*`tf.distribute.Strategy.experimental_run_v2` returns results from each local replica in the strategy, and there are multiple ways to consume this result. You can do `tf.distribute.Strategy.reduce` to get an aggregated value. You can also do `tf.distribute.Strategy.experimental_local_results` to get the list of values contained in the result, one per local replica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEaNCzYQvFqo"
   },
   "source": [
    "# Save the Model as a SavedModel\n",
    "\n",
    "You'll get a saved model of this finished trained model. You'll then \n",
    "need to zip that up to upload it to the testing infrastructure. We\n",
    "provide the code to help you with that here\n",
    "\n",
    "## Step 1: Save the model as a SavedModel\n",
    "This code will save your model as a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zAlTlRxrqFu"
   },
   "outputs": [],
   "source": [
    "model_save_path = \"./tmp/mymodel/1/\"\n",
    "tf.saved_model.save(model, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0Zfmx6LvTJA"
   },
   "source": [
    "## Step 2: Zip the SavedModel Directory into /mymodel.zip\n",
    "\n",
    "This code will zip your saved model directory contents into a single file.\n",
    "You can use the file browser pane to the left of colab to find mymodel.zip\n",
    "Right click on it and select 'Download'. It's a large file, so it might\n",
    "take some time.\n",
    "\n",
    "If the download fails because you aren't allowed to download multiple files from colab, check out the guidance here: https://ccm.net/faq/32938-google-chrome-allow-websites-to-perform-simultaneous-downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMuo2wQls41l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "zipf = zipfile.ZipFile('./mymodel.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('./tmp/mymodel/1/', zipf)\n",
    "zipf.close()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "TF3C2W4-1",
    "TF3C2W4-2",
    "TF3C2W4-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

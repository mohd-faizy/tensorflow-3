{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExerciseQuestion.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ1J1TgGffzb"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "Your job is to write the code for the distributed training of a flower classifier using the oxford flowers dataset. All of the preprocessing code is done for you, you just need to fill in the distributed training loop, fix some bugs, and when you have a model, there is code at the bottom for you to zip up your SavedModel and upload it to our testing infrastructure. After that you'll get a grade of pass or fail on this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Custom training with tf.distribute.Strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Import TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6W__qraV55"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmPfSVg_lGlb"
      },
      "source": [
        "# Note, if you get a checksum error when downloading the data\n",
        "# you might need to install tfds-nightly, and then restart \n",
        "!pip install tfds-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NsM-Bma5wNw"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "source": [
        "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
        "\n",
        "(train_examples, validation_examples, test_examples), info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True, split = splits)\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AXoHhrsbdF3"
      },
      "source": [
        "## Create a strategy to distribute the variables and the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mVuLZhbem8d"
      },
      "source": [
        "How does `tf.distribute.MirroredStrategy` strategy work?\n",
        "\n",
        "*   All the variables and the model graph is replicated on the replicas.\n",
        "*   Input is evenly distributed across the replicas.\n",
        "*   Each replica calculates the loss and gradients for the input it received.\n",
        "*   The gradients are synced across all the replicas by summing them.\n",
        "*   After the sync, the same update is made to the copies of the variables on each replica.\n",
        "\n",
        "Note: You can put all the code below inside a single scope. We are dividing it into several code cells for illustration purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VeZUWUj5S4"
      },
      "source": [
        "# If the list of devices is not specified in the\n",
        "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZngeM_2o0_JO"
      },
      "source": [
        "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k53F5I_IiGyI"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qb6nDgxiN_n"
      },
      "source": [
        "Export the graph and the variables to the platform-agnostic SavedModel format. After your model is saved, you can load it with or without the scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwJtsCQhHK-E"
      },
      "source": [
        "BUFFER_SIZE = num_examples\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUl3kUk8D5d"
      },
      "source": [
        "pixels = 224\n",
        "MODULE_HANDLE = 'https://tfhub.dev/tensorflow/resnet_50/feature_vector/1'\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGFit478BWD"
      },
      "source": [
        "def format_image(image, label):\n",
        "  image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "  return  image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7fj3GskHC8g"
      },
      "source": [
        "Create the datasets and distribute them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYrMNNDhAvVl"
      },
      "source": [
        "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
        "test_batches = test_examples.map(format_image).batch(1)\n",
        "\n",
        "train_dist_dataset = # YOUR CODE HERE TO MAKE A DISTRIBUTED DATASET\n",
        "val_dist_dataset = # YOUR CODE HERE TO MAKE A DISTRIBUTED DATASET\n",
        "test_dist_dataset = # YOUR CODE HERE TO MAKE A DISTRIBUTED DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXAo_wWbWSb"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "We use the Model Subclassing API to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "source": [
        "class ResNetModel(tf.keras.Model):\n",
        "  def __init__(self, classes):\n",
        "    super(ResNetModel, self).__init__()\n",
        "    self._feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                             trainable=False) \n",
        "    self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self._feature_extractor(inputs)\n",
        "    x = self._classifier(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iagoTBfijUz"
      },
      "source": [
        "# Create a checkpoint directory to store the checkpoints.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-wlFFZbP33n"
      },
      "source": [
        "## Define the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R144Wci782ix"
      },
      "source": [
        "with strategy.scope():\n",
        "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
        "  # global batch size.\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "  # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
        "  def compute_loss(labels, predictions):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8y54-o9T2Ni"
      },
      "source": [
        "## Define the metrics to track loss and accuracy\n",
        "\n",
        "These metrics track the test loss and training and test accuracy. You can use `.result()` to get the accumulated statistics at any time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt3AHb46Tr3w"
      },
      "source": [
        "with strategy.scope():\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuKuNXPORfqJ"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMmakq5EqeQ"
      },
      "source": [
        "# model and optimizer must be created under `strategy.scope`.\n",
        "with strategy.scope():\n",
        "  model = ResNetModel(classes=num_classes)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUQ_nAP1MtA9"
      },
      "source": [
        "with strategy.scope():\n",
        "  def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = # YOUR CODE HERE\n",
        "      loss = compute_loss(#YOUR CODE HERE)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "    return loss \n",
        "\n",
        "  def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    predictions = model(# YOUR CODE HERE)\n",
        "    t_loss = loss_object(# YOUR CODE HERE)\n",
        "\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZUAoY_DGv7"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX975dMSNw0e"
      },
      "source": [
        "with strategy.scope():\n",
        "  @tf.function\n",
        "  def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.run(#YOUR CODE HERE)\n",
        "\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                           axis=None)\n",
        " \n",
        "  @tf.function\n",
        "  def distributed_test_step(dataset_inputs):\n",
        "    return strategy.run(# YOUR CODE HERE)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    # TRAIN LOOP\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for x in tqdm(train_dist_dataset):\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    # TEST LOOP\n",
        "    for x in test_dist_dataset:\n",
        "      distributed_test_step(x)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
        "                \"Test Accuracy: {}\")\n",
        "    print (template.format(epoch+1, train_loss,\n",
        "                           train_accuracy.result()*100, test_loss.result(),\n",
        "                           test_accuracy.result()*100))\n",
        "\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1YvXqOpwy08"
      },
      "source": [
        "Things to note in the example above:\n",
        "\n",
        "* We are iterating over the `train_dist_dataset` and `test_dist_dataset` using  a `for x in ...` construct.\n",
        "* The scaled loss is the return value of the `distributed_train_step`. This value is aggregated across replicas using the `tf.distribute.Strategy.reduce` call and then across batches by summing the return value of the `tf.distribute.Strategy.reduce` calls.\n",
        "* `tf.keras.Metrics` should be updated inside `train_step` and `test_step` that gets executed by `tf.distribute.Strategy.experimental_run_v2`.\n",
        "*`tf.distribute.Strategy.experimental_run_v2` returns results from each local replica in the strategy, and there are multiple ways to consume this result. You can do `tf.distribute.Strategy.reduce` to get an aggregated value. You can also do `tf.distribute.Strategy.experimental_local_results` to get the list of values contained in the result, one per local replica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEaNCzYQvFqo"
      },
      "source": [
        "# Save the Model as a SavedModel\n",
        "\n",
        "You'll get a saved model of this finished trained model. You'll then \n",
        "need to zip that up to upload it to the testing infrastructure. We\n",
        "provide the code to help you with that here\n",
        "\n",
        "##Step 1: Save the model as a SavedModel\n",
        "This code will save your model as a SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zAlTlRxrqFu"
      },
      "source": [
        "model_save_path = \"/tmp/mymodel/1/\"\n",
        "tf.saved_model.save(model, model_save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Zfmx6LvTJA"
      },
      "source": [
        "##Step 2: Zip the SavedModel Directory into /mymodel.zip\n",
        "\n",
        "This code will zip your saved model directory contents into a single file.\n",
        "You can use the file browser pane to the left of colab to find mymodel.zip\n",
        "Right click on it and select 'Download'. It's a large file, so it might\n",
        "take some time.\n",
        "\n",
        "If the download fails because you aren't allowed to download multiple files from colab, check out the guidance here: https://ccm.net/faq/32938-google-chrome-allow-websites-to-perform-simultaneous-downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuo2wQls41l"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "\n",
        "zipf = zipfile.ZipFile('/mymodel.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('/tmp/mymodel/1/', zipf)\n",
        "zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}